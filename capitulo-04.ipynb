{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lidando com diferentes layout de sites\n",
    "O código abaixo, mostra um exemplo de uma classe \"Content\" e duas funções de coleta de dados que aceitam um objeto BeautifulSoup e devolvem uma instância de Content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:   1º navio de grãos liberado após acordo entre Rússia e Ucrânia é rejeitado por atraso   \n",
      "URL: https://noticias.uol.com.br/ultimas-noticias/ansa/2022/08/08/comprador-rejeita-1-carga-de-graos-que-saiu-da-ucrania.htm\n",
      "Notícias do conflito entre Rússia e Ucrânia\n",
      "08/08/2022 18h15 \n",
      "A primeira carga de grãos de milho que saiu da Ucrânia após o acordo com a Rússia para o desbloqueio do Mar Negro foi rejeitada pelo comprador e agora aguarda um novo interessado na compra, informou a embaixada do país em Beirute nesta segunda-feira (8).\n",
      "As 26 mil toneladas do produto deveriam ter sido desembarcadas neste domingo (7), mas o navio Razoni ainda continua próximo ao mar turco. Por conta do adiamento, a embarcação não pode entrar no porto de Beirute, no Líbano.\n",
      "\n",
      "O comprador teria desistido do produto pelo atraso na entrega, de mais de cinco meses. Agora, a embaixada informou que está em busca de um novo comprador - que pode estar no Líbano ou em algum outro país da região.\n",
      "O bloqueio aos portos ucranianos foi iniciado no primeiro dia da invasão da Rússia, em 24 de fevereiro, e só chegou ao fim após um acordo firmado entre Kiev e Moscou, com intermediação de Ancara e da Organização das Nações Unidas, em 22 de julho.\n",
      "Desde o dia 1º de agosto, quando o Rizoma deixou o porto de Odessa, outros cinco navios deixaram o mesmo local e os portos de Chornomorsk e Pivdennyi. Inclusive nesta segunda, a embarcação Polarnet, de bandeira turca, já chegou ao porto de Derince e está descarregando sua carga.Conforme dados de Kiev, há cerca de 20 milhões de toneladas de grãos e cereais parados nos portos ucranianos desde fevereiro aguardando o embarque.\n",
      " ID: {{comments.info.id}}URL: {{comments.info.url}}\n",
      "Por favor, tente novamente mais tarde.\n",
      "\n",
      "Não é possivel enviar novos comentários.\n",
      "Essa área é exclusiva para você, assinante, ler e comentar.\n",
      "Ainda não é assinante? Assine já.\n",
      "Se você já é assinante do UOL, faça seu login.\n",
      "O autor da mensagem, e não o UOL, é o responsável pelo comentário. Reserve um tempo para ler as Regras de Uso do UOL.\n",
      "\n",
      "\n",
      "Wálter Maierovitch\n",
      "Alexandre Saconi\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Content:\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "\n",
    "def getPage(url):\n",
    "    req = requests.get(url)\n",
    "    return BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "def scrapeNoticiasUol(url):\n",
    "    bs = getPage(url)\n",
    "    title = bs.find(\"h1\").text\n",
    "    lines = bs.find_all(\"p\")\n",
    "    body = '\\n'.join([line.text for line in lines])\n",
    "    return Content(url, title, body)\n",
    "\n",
    "def scrapeBrookings(url):\n",
    "    bs = getPage(url)\n",
    "    title = bs.find(\"h1\").text\n",
    "    body = bs.find(\"div\", {\"class\", \"post-body\"}).text\n",
    "    return Content(url, title, body)\n",
    "\n",
    "\"\"\" url = 'https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/'\n",
    "content = scrapeBrookings(url)\n",
    "print('Title: {}'.format(content.title))\n",
    "print('URL: {}'.format(content.url))\n",
    "print(content.body) \"\"\"\n",
    "\n",
    "\n",
    "# Obs: No livro o autor usou o site do The New York Times, porém agora o site\n",
    "# Solicita que o leitor adquira um plano de leitura.\n",
    "# Para trazer algo que funcione ao código adaptei o código para o Uol Notícias\n",
    "url = 'https://noticias.uol.com.br/ultimas-noticias/ansa/2022/08/08/comprador-rejeita-1-carga-de-graos-que-saiu-da-ucrania.htm'\n",
    "content = scrapeNoticiasUol(url)\n",
    "print('Title: {}'.format(content.title))\n",
    "print('URL: {}'.format(content.url))\n",
    "print(content.body)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desenvolvendo um Crawler para coletar o título e informações do conteúdo de qualquer site, utilizando 2 classes que pode poupar códigos repetidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Content:\n",
    "    \"\"\"\n",
    "        Classe base comum para todos os artigos/páginas\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "        \n",
    "    def print(self):\n",
    "        \"\"\"\n",
    "            Uma função flexível de exibição controla a saída\n",
    "        \"\"\"\n",
    "        print(\"URL: {}\".format(self.url))\n",
    "        print(\"TITLE: {}\".format(self.title))\n",
    "        print(\"BODY: {}\".format(self.body))\n",
    "            \n",
    "class Website:\n",
    "    \"\"\"\n",
    "        Contém informações sobre a estrutura do site\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name, url, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag\n",
    "        \n",
    "# As classes acima servirão de base para de fato eu construir o crawler utilizando agora as 2 classes criadas acima\n",
    "        \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Crawler:\n",
    "    \n",
    "    def getPage(self, url):\n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "        return BeautifulSoup(req.text, 'html.parser')\n",
    "    \n",
    "    def sageGet(self, pageObj, selector):\n",
    "        \"\"\"\n",
    "            Função utilitária usada para obter uma string de conteúdo de um objeto BeautifulSoup e um seletor.\n",
    "            Devolve uma string vazia caso nenhum objeto seja encontrado para o dado seletor\n",
    "        \"\"\"\n",
    "        selectedElems = pageObj.select(selector)\n",
    "        if selectedElems is not None and len(selectedElems) > 0:\n",
    "            return '\\n'.join([elem.get_text() for elem in selectedElems])\n",
    "        \n",
    "        return ''\n",
    "    \n",
    "    def parse(self, site, url):\n",
    "        \"\"\"\n",
    "            Extrai conteúdo de um dado URL de página\n",
    "        \"\"\"\n",
    "        \n",
    "        bs = self.getPage(url)\n",
    "        if bs is not None:\n",
    "            title = self.sageGet(bs, site.titleTag)\n",
    "            body = self.sageGet(bs, site.bodyTag)\n",
    "            if title != '' and body != '':\n",
    "                content = Content(url, title, body)\n",
    "                content.print()\n",
    "\n",
    "\n",
    "# Abaixo o código que instância os objetos criados acima, e dar início ao processo\n",
    "\n",
    "crawler = Crawler()\n",
    "\n",
    "siteData = [\n",
    "    ['O\\'Reilly Media', 'http://oreilly.com', 'h1', 'section#product-description'],\n",
    "    ['Reuters', 'http://reuters.com', 'h1', 'div.StandardArticleBody_body_1gnLA'],\n",
    "    ['Brookings', 'http://www.brookings.edu', 'h1', 'div.post-body']\n",
    "]\n",
    "\n",
    "websites = []\n",
    "for row in siteData:\n",
    "    websites.append(Website(row[0],row[1],row[2],row[3]))\n",
    "    \n",
    "\n",
    "crawler.parse(websites[0], 'http://shop.oreilly.com/product/0636920028154.do')\n",
    "crawler.parse(websites[1], 'http://www.reuters.com/article/us-usa-epa-pruitt-idUSKBN19W2D0')\n",
    "crawler.parse(websites[2], 'http://www.brookings.edu/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/')\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3292aac4fa142a3d6f51f934fe9bab4eae34c6a6f75ad15485f78f7da0f16cb4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
